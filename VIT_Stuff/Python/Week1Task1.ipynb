{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf1LMhHqPKVT"
      },
      "source": [
        "# Week 1 - Task 1 | GroundZero AI\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-cZ1qhe4Xh2"
      },
      "source": [
        "### Import the necessary stuff!\n",
        "\n",
        "Let's first import all the libraries we are going to use in this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpVAeaVXl07l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEwFA8KV4lzL"
      },
      "source": [
        "# Part 1 - How is RNN limited? (Context and Vanishing Gradient)\n",
        " As you have learnt RNNs lose context over time, let's try to visualize that, through an example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3_nVLps4lE0"
      },
      "outputs": [],
      "source": [
        "### CODE STARTS HERE ###\n",
        "\n",
        "# enter the length of sequence here, variable name 'sequence_length' | you are supposed to take the values 20,100,1000,2000 and compare the results\n",
        "sequence_length=20\n",
        "### CODE ENDS HERE ###\n",
        "\n",
        "# generate a sequence of values from the sine function and create sequence_length evenly spaced values between 0 and 4(pi)\n",
        "data = np.sin(np.linspace(0, 4 * np.pi, sequence_length))\n",
        "#adjusts the shape of the 1D array into a 3D array\n",
        "data = data.reshape((1, sequence_length, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUEQtmNHLgiJ"
      },
      "outputs": [],
      "source": [
        "# Create a simple RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, sequence_length):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=1, hidden_size=10, batch_first=True)\n",
        "        self.dense = nn.Linear(10, 1)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        return self.dense(x)\n",
        "rnn_model = RNNModel(sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pcbe6uqYLzX0"
      },
      "outputs": [],
      "source": [
        "#train it\n",
        "optimizer = Adam(rnn_model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "data_tensor = torch.FloatTensor(data)\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = rnn_model(data_tensor)\n",
        "    loss = criterion(outputs, data_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "#find out the rnn outputs\n",
        "with torch.no_grad():\n",
        "    predictions = rnn_model(torch.FloatTensor(data)).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DWlnA6bMYVi"
      },
      "outputs": [],
      "source": [
        "#plot the original sequence vs rnn outputs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data[0, :, 0], label=\"Original Sequence\", marker='o')\n",
        "plt.plot(predictions[0, :, 0], label=\"RNN Output\", linestyle='--', marker='x')\n",
        "plt.title(\"RNN and Context Retention\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us-k9pfnMyrD"
      },
      "source": [
        "## Takeaways\n",
        "Notice as you increase the length of the sequence of words, the gap between the original sequence and the final RNN output increases. This shows us how contextual information is often lost in RNNs while processing long sequences\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgVTbkqfNKMg"
      },
      "source": [
        "Now let's talk about how gradient starts to vanish after traning it for longer\n",
        "epochs, thus after a long time, it becomes close to zero!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohzwyXfNZOl"
      },
      "outputs": [],
      "source": [
        "#define a vanishing gradient rnn model\n",
        "class VanishingRNNModel(nn.Module):\n",
        "    def __init__(self, sequence_length):\n",
        "        super().__init__()\n",
        "        self.rnn1 = nn.RNN(input_size=1, hidden_size=10, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(input_size=10, hidden_size=10, batch_first=True)\n",
        "        self.rnn3 = nn.RNN(input_size=10, hidden_size=10, batch_first=True)\n",
        "        self.dense = nn.Linear(10, 1)\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn1(x)\n",
        "        x, _ = self.rnn2(x)\n",
        "        x, _ = self.rnn3(x)\n",
        "        return self.dense(x)\n",
        "vanishing_rnn_model = VanishingRNNModel(sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmHlReJrNe5d"
      },
      "outputs": [],
      "source": [
        "# select the optimizers and loss function\n",
        "optimizer = Adam(vanishing_rnn_model.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SkZee0qNo7w"
      },
      "outputs": [],
      "source": [
        "### CODE STARTS HERE ###\n",
        "\n",
        "# train the model on 10,20,50 epochs and see how the graph looks everytime | store the result in a variable named 'history'\n",
        "num_epochs = 50\n",
        "history = {'loss': []}\n",
        "data_tensor = torch.FloatTensor(data)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = vanishing_rnn_model(data_tensor)\n",
        "    loss = criterion(outputs, data_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    history['loss'].append(loss.item())\n",
        "### CODE ENDS HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu7t5zpqN-t7"
      },
      "outputs": [],
      "source": [
        "# plot the loss to observe vanishing gradient\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['loss'], label=\"Training Loss\", marker='o')\n",
        "plt.title(\"Vanishing Gradient in Deeper RNNs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgmko-U7OCD8"
      },
      "source": [
        "Notice how at longer epochs, the graph edges very close to zero, hence we have the problem of using RNNs for longer training sessions!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV2YCItsOQvK"
      },
      "source": [
        "# Part 2 - Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ITHsaByUsWt"
      },
      "source": [
        "Now let's try to code up the input embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO4OFMhuUr2A"
      },
      "outputs": [],
      "source": [
        "# we input any sentence we want\n",
        "sentence = \"Transformers have revolutionized natural language processing and machine learning.\" #please feel free to add your sentence as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvimlgwaWLmJ"
      },
      "outputs": [],
      "source": [
        "#create a vocabulary\n",
        "vocab = set(sentence.split())\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6_JcRldV5fv"
      },
      "outputs": [],
      "source": [
        "### CODE STARTS HERE ###\n",
        "word_to_index = #input here - write a dictionary that maps every word to its index\n",
        "index_to_word = #input here - vice versa of the above\n",
        "tokens = #input here - assign each word of the sentence to it's tokens\n",
        "### CODE ENDS HERE ###\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMdhS2GMWsn-"
      },
      "outputs": [],
      "source": [
        "# we create an embedding layer\n",
        "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=8)\n",
        "\n",
        "# we find the embedding by passing our tokens through the embedding layer\n",
        "embeddings = embedding_layer(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsy0ef9EXAnx"
      },
      "outputs": [],
      "source": [
        "#let's see what the embeddings look like\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk40SE-mW9Ng"
      },
      "outputs": [],
      "source": [
        "#visualize the embeddings\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(sentence_tokens.shape[1]):\n",
        "    plt.scatter(range(embeddings.shape[2]), embeddings[0, i, :], label=f\"Word: {index_to_word[sentence_tokens[0, i]]}\" if i < 5 else \"\")\n",
        "plt.title(\"Input Embeddings\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTm11FfoPiE9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part 3 - Positional Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGAlDMoCPk1C"
      },
      "source": [
        "In this section, we code up the positional embedding values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udS48tt_PsbM"
      },
      "source": [
        "The positional embedding formulas are given as follows:\n",
        "For a position (pos) and embedding dimension (i):\n",
        "\n",
        "\n",
        "$$PE_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)$$\n",
        "$$PE_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPA_GS4wPkSh"
      },
      "outputs": [],
      "source": [
        "### CODE STARTS HERE ###\n",
        "positions = #input here - hint : use np.arange\n",
        "denominator = #input here - code up the denominator terms\n",
        "pos_embed =  #input here - initialize everything as zero\n",
        "#fill pos_embed with the correct values\n",
        "### CODE ENDS HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0Cwo58KS7L5"
      },
      "outputs": [],
      "source": [
        "#let us check what the first 5 positional embeddings are\n",
        "print(pos_embed[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxrgX6OKSVp7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Part 4 - Encoder Input\n",
        "\n",
        "We finally add up the input embeddings we made and positional embeddings to get the final input for the encoders.\n",
        "\n",
        "$$Encoder \\space input = input \\space embeddings + positional \\space embeddings$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oC8yGeXSuBJ"
      },
      "outputs": [],
      "source": [
        "### CODE STARTS HERE ###\n",
        "final_input = #input here\n",
        "### CODE ENDS HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-f6Y3naSz0o"
      },
      "outputs": [],
      "source": [
        "#let us check what the final input is\n",
        "print(final_input)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
