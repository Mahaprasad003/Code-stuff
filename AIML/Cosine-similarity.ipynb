{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "562343c9-c6a8-4066-918c-8d5ad78866d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4162f42-fac4-44d6-af1c-0d2c72807f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Julie loves India more than Linda loves India\"\n",
    "sent2 = \"Jane likes India more than Julie loves India\"\n",
    "\n",
    "# sent1 = \"Mahaprasad loves icecream more than he hates pizza\"\n",
    "# sent2 = \"Mahaprasad is a good boy and obedient child\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22283cb1-a818-4023-837b-ec0f13ecc2d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Find the cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6b623e-8dc0-471b-875b-2e30abb8c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cos_similarity(sent_a, sent_b):\n",
    "    # make a list of unique words\n",
    "    list_words = []\n",
    "    for word in sent_a.split():\n",
    "        if word not in list_words:\n",
    "            list_words.append(word)\n",
    "    for word in sent_b.split():\n",
    "        if word not in list_words:\n",
    "            list_words.append(word)\n",
    "            \n",
    "    # create 2 vectors for the words\n",
    "    vector_a = []\n",
    "    for word in list_words:\n",
    "        vector_a.append(sent_a.count(word))\n",
    "    vector_b = []\n",
    "    for word in list_words:\n",
    "        vector_b.append(sent_b.count(word))\n",
    "    \n",
    "    #find the dot product\n",
    "    dot_product = 0\n",
    "\n",
    "    for i in range(len(vector_a)):\n",
    "        dot_product += vector_a[i] * vector_b[i]\n",
    "        \n",
    "    mod_a = 0\n",
    "    sum = 0\n",
    "    for i in vector_a:\n",
    "        # print(i**2)\n",
    "        sum += (i**2)\n",
    "    mod_a = pow(sum, 0.5)\n",
    "    \n",
    "    mod_b = 0\n",
    "    sum = 0\n",
    "    for i in vector_b:\n",
    "        sum += (i**2)\n",
    "    mod_b = pow(sum, 0.5)\n",
    "    mod_b\n",
    "\n",
    "    cos_theta = dot_product/(mod_a * mod_b)\n",
    "    \n",
    "    return cos_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697ed2cc-608c-4575-8808-2a33b9944beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8215838362577491"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"I am a disco dancer and what are you\"\n",
    "y = \" x\"\n",
    "find_cos_similarity(sent1, sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca07825d-9673-4257-ae7b-d4655cc111c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting file names from folder\n",
    "file_names = glob.glob('./data/English_text/*.txt')\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb2f0ec-c309-4735-b510-a7b8894e32f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = []\n",
    "\n",
    "for elem in range(len(file_names)):\n",
    "    try:\n",
    "        with open(file_names[elem], \"r\") as file:\n",
    "            # print(elem)\n",
    "            data = file.read()\n",
    "            strings.append(data)\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "# with open(file_names[0], \"r\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "len(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d1d7e3-ed13-4150-a817-5b69a4202c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ascii', 'Windows-1252', 'UTF-8-SIG', 'ISO-8859-1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chardet import detect\n",
    "\n",
    "types = []\n",
    "for elem in range(len(file_names)):\n",
    "    with open(file_names[elem], 'rb') as file:\n",
    "        data = file.read()\n",
    "        encoding_type = detect(data)['encoding']\n",
    "        \n",
    "        if encoding_type not in types:\n",
    "            types.append(detect(data)['encoding'])\n",
    "\n",
    "types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b1f70-76e7-4b5e-a4ba-03a300b59d10",
   "metadata": {},
   "source": [
    "## Creating a wrangle function to get data and give in the specific manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c449144b-5323-4691-b735-1bbb682e4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(file_names):\n",
    "    strings = []\n",
    "    for elem in range(len(file_names)):\n",
    "        try:\n",
    "            with open(file_names[elem], \"r\") as file:\n",
    "                data = file.read()\n",
    "                strings.append(data)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # Remove symbols, numbers, and convert to lowercase\n",
    "    cleaned_strings = [re.sub('[^A-Za-z]+', ' ', text).lower() for text in strings]\n",
    "    \n",
    "    print(f\"Successfully read {len(strings)} files, removed symbols, numbers, and converted to lowercase.\")\n",
    "    return cleaned_strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3bd03b3-d871-4612-9933-d0d655475113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 133 files, removed symbols, numbers, and converted to lowercase.\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = wrangle(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2776e1bc-fe35-4e6b-befe-90c0982c61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(list_strings):\n",
    "    word_list = []\n",
    "    for long_string in range(len(list_strings)):\n",
    "        words = list_strings[long_string].split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in word_list:\n",
    "                word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df75bf19-78b4-48d9-84d9-d86d049cfdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = unique_words(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d29624b-5455-4787-9941-926f2aef5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_word_to_index(vocabulary):\n",
    "    \n",
    "#     word_to_index = {}\n",
    "\n",
    "    \n",
    "#     for index, word in enumerate(vocabulary):\n",
    "        \n",
    "#         word_to_index[word] = index\n",
    "\n",
    "    \n",
    "#     return word_to_index\n",
    "\n",
    "word_to_index = {word: index for index, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59227057-e208-4f5a-8f01-6ccd1e956549",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_word_to_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_to_index \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_word_to_index\u001b[49m(vocabulary)\n\u001b[0;32m      2\u001b[0m word_to_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaipur\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_word_to_index' is not defined"
     ]
    }
   ],
   "source": [
    "word_to_index = create_word_to_index(vocabulary)\n",
    "word_to_index['jaipur']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce13539-d410-4069-9b09-3174c475228d",
   "metadata": {},
   "source": [
    "# convert each string to it's vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77e92c8c-7b24-4200-b1be-8a2dd5d413e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = []\n",
    "\n",
    "for document in tokenized_corpus:\n",
    "    vector = [0] * len(vocabulary)\n",
    "    for word in document:\n",
    "        if word in word_to_index:\n",
    "            vector[word_to_index[word]] +=1\n",
    "    document_vectors.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "367f80af-07ef-4569-8002-96dc27e8a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24523\n"
     ]
    }
   ],
   "source": [
    "# print(sum(document_vectors[30]))\n",
    "vocabulary.index('jaipur')\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e73341ed-aee2-42bb-bfb3-6b6ca27b84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(document_vectors[0])):\n",
    "    if document_vectors[0][i] == 8:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64f71d9f-e051-435a-a940-33ed472698b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a0f8a124-5fb5-4a52-87d0-dddcf44546f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus[0].split().count('1929')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f4cba6b4-a44f-4500-958d-16f955d832d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chand'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a13d00b-7286-4a59-b152-94b8b350cf8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['jaipur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36899361-17bf-46bc-ad44-985e9430ec2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666e3ff8-78bf-4c10-a0d4-e884f9dfc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(tokenized_corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753c9b01-1ebe-48d5-b774-a47357a2a6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 23258)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fbc6a7b-af97-4f2c-9b32-2f6fff21181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_array = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae037f8-d354-4d18-adde-a589cd6dcf95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23258"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dense_array[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22861d5-44dc-43d1-93c6-50ffb9debbcc",
   "metadata": {},
   "source": [
    "# Finding euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6666301-558a-40bc-80c1-0fa58b134209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    if len(vec1) != len(vec2):\n",
    "        raise ValueError(\"Vectors must have the same length\")\n",
    "    \n",
    "    squared_distance = 0\n",
    "    for i in range(len(vec1)):\n",
    "        squared_distance += (vec1[i] - vec2[i]) ** 2\n",
    "    \n",
    "    return squared_distance ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37d6b715-d114-4b18-bacd-76e70a2c8a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 109, 123]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_random_vectors = 3\n",
    "random.seed(10)\n",
    "random_indices = random.sample(range(X.shape[0]), num_random_vectors)\n",
    "random_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35faae58-178c-4bb2-bce9-06053bcf44f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_vectors = [X[i].toarray().ravel() for i in random_indices]\n",
    "selected_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "939efc77-1633-482e-81d4-33cfdebcb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_distances = []\n",
    "for i, document_vector in enumerate(X):\n",
    "    distances = [euclidean_distance(document_vector.toarray().ravel(), selected_vector) for selected_vector in selected_vectors]\n",
    "    euclidean_distances.append(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8ef705c-9214-48a3-b549-21f9a77adefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115.2432210587677, 150.73818361649447, 128.2926342390708]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_distances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc675650-4d21-4da9-b131-8a1a6e0cddbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Iterate through the documents and their corresponding distances\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m distances \u001b[38;5;129;01min\u001b[39;00m euclidean_distances:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Find the index (vector) with the smallest distance using NumPy argmin\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m     closest_vector \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Assign the document to the corresponding group based on the closest vector\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closest_vector \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1312\u001b[0m, in \u001b[0;36margmin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1311\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "euclidean_distances = np.array(euclidean_distances)\n",
    "\n",
    "\n",
    "group1 = []\n",
    "group2 = []\n",
    "group3 = []\n",
    "\n",
    "# Define a small threshold for convergence\n",
    "threshold = 1\n",
    "\n",
    "# Initialize flags for convergence\n",
    "converged1 = False\n",
    "converged2 = False\n",
    "converged3 = False\n",
    "\n",
    "# Iterate until all groups converge\n",
    "while not (converged1 and converged2 and converged3):\n",
    "    # Reset group lists\n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    group3 = []\n",
    "    \n",
    "    # Iterate through the documents and their corresponding distances\n",
    "    for distances in euclidean_distances:\n",
    "        # Find the index (vector) with the smallest distance using NumPy argmin\n",
    "        closest_vector = np.argmin(distances)\n",
    "        \n",
    "        # Assign the document to the corresponding group based on the closest vector\n",
    "        if closest_vector == 0:\n",
    "            group1.append(distances)\n",
    "        elif closest_vector == 1:\n",
    "            group2.append(distances)\n",
    "        elif closest_vector == 2:\n",
    "            group3.append(distances)\n",
    "    \n",
    "    # Calculate the means of the groups using NumPy\n",
    "    mean1 = np.mean(group1, axis=0)\n",
    "    mean2 = np.mean(group2, axis=0)\n",
    "    mean3 = np.mean(group3, axis=0)\n",
    "    \n",
    "    # Check for convergence using NumPy and threshold\n",
    "    converged1 = np.all(np.abs(mean1 - mean2) < threshold)\n",
    "    converged2 = np.all(np.abs(mean2 - mean3) < threshold)\n",
    "    converged3 = np.all(np.abs(mean3 - mean1) < threshold)\n",
    "\n",
    "# Now, group1, group2, and group3 contain documents grouped based on the closest vector,\n",
    "# and they have converged means.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766839f-0acc-47ac-a8ae-dc3e1c6d460d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
